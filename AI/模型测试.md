# 小模型

小模型和大模型的话它测评的测试的一个侧、测试集、测试报告和测试结果这些。他基本上都是不一样的。



小模型的话，根据业务和任务不同，可以选择在训练、部署、发布任一阶段后进行一次评测，也可以不做，看具体的业务，主要还是在速度测试和精度测试







然后小模型的话，我已经有点儿忘了，然后我记得大概就是他是就是就是训练之后，部署之前会有一轮儿测试，然后部署中间会有一些测试。然后那个部署之后，就是那个也会就是发布，就是部署之后你可以选择发布，然后你你，但是一般大家还是会做一些测试，然后这个具体他的测试到底要做好几轮儿，还是说只做。那个一两轮儿的话，这个都是根据就是任务和不同的侧重来看的。





然后小模型测试的话，主要还是两类，一类是速度测试，一类是精度测试，然后这边的话就是为什么它会有这么多类测试呢？一个是我们跑完训练之后，本身就要对它做一轮儿测试嘛，这个测试主主要是看一下那个。针对一个模型能力，然后我们觉得模型能力OK了之后，我们才会进入到一些部署环节嘛，然后部署环节这边牵扯到的话，它里面的东西就比较多，就像那个那个jad里不是会写了一些就是。他不是会说要你掌握一些什么量化、减脂、蒸馏这些吗？这些都是属于部署里面的一些，它是一些优化项，比如说就是我们的模型，它会有一些计算强度很大，然后比较模型文件也比较大，比较吃。内存。



然后它模型文件比较大，比较吃内存，然后它速度比较慢的话，像这些就是量化减值，这些它都是一个是优化它的一个计算强度，然后让它模型变小，然后模型文件大小变小，然后。做一些速度上的加速的这样一些能力。



然后这个过程中呢，可能会牵扯到一些，这个主要是看你用的什么样的一个量化方法，或者是说是其他的一些手法，他可能会牵扯到一些精度的损失。然后这个后面的话，我们也需要测一下，看他有没有精度大范围的损失，然后如果有损失的话，还要做一轮儿精度对齐，然后这里面会牵扯到一些测试啊，比如说还会有一些就是。模型转换，因为你会把模型发到那个移动端嘛，然后像这个过程中会牵扯到一些模型转换，然后转换可能也会有一些进度损失，然后需要你做一些测试和进度对齐之类的工作。





然后等这些都搞完了，就是等到最后要发布的时候，其实我们总体来看的话，就是会做一下，就是他测评的话，就会更偏重于一个，嗯，就是整体看的一个效果了，这个的话。一个是看它的速度，一个是那个速度的话，这边有整体的一个处理速度和推理的一个速度，然后都会比较关注，然后另一个的话就是会看一些指标，就是我刚刚说的那个精度测试，也可以说。就是这个指标看啥的话，也是跟那个业务相关，比如说的话就是如果是分类任务的话，可能就会看一下召回率这些，然后目标检测任务的话，可能会看一下MMP。





这个这一块儿的测评，主要都是发到那个终端去做了，然后就像就是你刚刚说的那一些，比如说像以前的adela呀，像out link呀，像SEP呀，这一些它的测试的话，它不是说它是做的模型测试。就是我们说的模型测评本身它是一些脚本，就是它是写代码，然后做那个模型的，它的一些那个测试，然后这些平台其实它是不具备，本身是不具备一个测试能力的，它可能只会给你提供一些硬件啊，然后。他除非就是他集成了你的脚本，就像out link一样，Out link它其实是只是提供一个工程化的一个能力，他并没有说他是真的可以做模型测试，他其实不能，他只是用了用户的代码跑的用户。





他只是就是跑的用户的代码，然后让用户做的一个模型测试，只只是说提供了一个前端，但是他其并没有测试能力，所以说这些的话就不能叫做模型测试。他其实只是一个就是平台化的一个能力。





就是模型测评它的那个工作内容的话，就是像我刚刚说的这一块儿，他可能他那做测评的人肯定也都懂部署，就是一般来说做测评的人一般都会做部署。所以说就会要知道就是量化呀，转换啊，那个蒸馏这些具体怎么做。



# 大模型

然后大模型这边的话，首先就是需要了解一下大模型它整个的流程，它其实跟小模型就都就不是很一样了，它的话就是前最主要的前期。就是就是前面可能不太一样的，就是他那个有了数据集之后，他是先做预训练，然后预训练出了一个预训练的模型之后，可能会有一些续训的工作，这个看情况看主要是你想侧重于哪方向的大模型，然后在这个基础上，然后得到的一个完整的一个预训练模型之后，然后你可。嗯，就根据你的一个业务需求做一下微调，然后微调之后，然后可能做那个一个那个推理，然后和部署这些相关的方向了。





然后这个测试呢，就是这个测评呢，也是就是遍布全流程的，比如说你预训练完了之后，你可能也想测评一下你的那个预训练的这个模型的一个效果，然后你可能。就是微调之后，你可能也会想要测评一下你这个微调模型的一个效果，然后最后交付的时候，可能也会想要测试一下那个你的一个效果。



然后这边大模型的总体的一个它的一个测试，测评的一个思路，主要就是它是一个主观测评和客观测评。



我先说一下客观测评吧，客观测评主要是用一些市面上主流的一些测评级，比如说一些就是像比就是一些主流的，就是全全球通用的一些测评网站，他们会提供一些主流的测评级。来给你的模型打分，比如说像mm Lu和CE value那些网站会提供相关的一些测评数据集，然后这个数据集呢，一般来说就是知名网站的客观数据集都会拿来，就是做一轮儿打分，然后这个叫客观测试。



然后主观测试这边呢，它就是，嗯，就是自己拼的一个数据集了，这个主要是看你的一个怎么说呢，就是你的模型的一个倾向吧，你需要自己来调一个主观的数据集。



比如说有那个，它是属于金融大模型，还是属于营销大模型，还是属于代码大模型，这些可能它的一个侧重点就不一样。但是他会有一个基础能力的一个测试，你肯定这个主观数据集的话，你肯肯定都会涉及到，比如说像指令啊，像写作呀，像数理推理能力呀，像代码生成能力呀，像工具调用能力呀，像那个。类人聊天能力呀，还有多人对话能力呀，像这一些的话，肯定都要方方面面涉及到，然后他的一个数据集的一个比重，或者是说那个数据集该怎么分布的话，这个就根据你模型最终想要的一个能力来。



然后最终的一个测试报告的话，是必须要涵盖你的模型本身，你的模型的那个版本号呀，还有你模型厂商啊，还有然后最重要的后面的一些具体内容就是你的客观级打分和主观级打分了。



客观级打分的话，其实你就只需要就是我刚刚说的那些客观数据级，像C1VALUE这些，你把它跑一轮，把分儿跑出来，然后就可以写上了，像主观测试这边的话，一般来说，现在比如说像商汤这边，研究院这边的话，它主要的一个覆盖的场景。就是比如说有知识问答，有写作能力，有那个闲聊情商，有人物风格，有数理能力，然后有代码生成，然后有一些安全测试，然后像这样的分布，然后。每一每一类都有专门的一个测评节，然后他进行一个测试，然后最后出来的一个汇总，然后进行一个打分，然后这个打分呢，也比较有主观倾向，他比如说比如说他可能就是。打



答对了一个问题给一分，然后答对问题的时候，他有可能给你胡说八道了另一些东西，然后可能就要倒扣分，然后他答对问题的同时。可能还给你补充说明了一些新的东西，而这些东西也是对的，那可能就会有一些加分，这个就看那个就是有一些主观化的一个评判标准了。



最好就直说自己有AI平台测试经验，了解ai大模型/小模型测评









那这个感觉要求蛮高的，就像我刚刚说嘛，就是你不同任务它测试的一个看的一个标准是不一样的，这个需要根据经验来，就像我刚刚说目标检测任务可能更关注一个map，它的。指标，然后比如说2D分类任务，他更关注召回率和那个准确度，然后像这些的话，都是你做过很多，然后经验性的得出来的东西。然后像这些的话，你如果要设计测力，或者是设计测试的一个标准化的东西的话，你要根据他的不同任务，以及他的不同要求，然后要规划出来不同的一个测试报告，他应该长啥样。还有他你可能也要设计一下，就是比如说我们需要在哪些流程中来介入测试。



初级研究员估计做不了，因为初级研究员他可能只focus在一个任务，他可能没有那么多任任务的一个经验，这个应该是你必须了解就是。很多业务他的一个试用场景了之后，你可能会得出一个结论。



然后比如说如果是大模型的话，可能你要根据比如说大模型的它的一个应用场景，它是手机，手机的一个应用呢，还是说是一个工具的一个调用呢，然后还有它那个大模型是属于就刚刚说的像是属于营销大模型，还是拟人对话大模型，然后它的一个那个能力的话，知识分布的话，它是比如说它数理推理呀，还是写作这一些。你可能根据这一些的话，你来不同的判评判一个测试标准，然后你来定一个标准的一个测试体系。



这种可能都要求你对每一类任务和每一类模型都要有一个非常深入的了解，然后你才知道我具体应该关注什么样的指标，我的测试报告最终应该长啥样。







深度学习平台：

集大规模AI算力管理、专业的AI工具链、开放式AI算法于一体的工业级AI生产开放平台，实现从数据标注，算法设计，到模型训练、部署的全链路、批量化过程。

模型压缩工具：将大模型转化为轻量级模型，消耗内存更少、

训练数据平台：涵盖训练数据集的生命周期全过程

SenseParrots训练框架：高效利用算力，

跨平台模型部署工具：提升推理效率